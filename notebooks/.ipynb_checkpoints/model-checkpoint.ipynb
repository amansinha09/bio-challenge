{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "#rnncharmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description=\"Running ner...\")\n",
    "parser.add_argument('--device', default = torch.device('cpu'), \n",
    "                    help='cpu or gpu')\n",
    "parser.add_argument('--hs', default=768, type=int,\n",
    "                    help='Hidden layer size')\n",
    "parser.add_argument('--bs', default=32, \n",
    "                    help='batch size')\n",
    "parser.add_argument('--nl', default=2, \n",
    "                    help='Number of layers')\n",
    "parser.add_argument('--bidir', default=1,\n",
    "                   help='bi directional')\n",
    "parser.add_argument('--inplen', default=50,\n",
    "                   help='sequence/sentence length')\n",
    "parser.add_argument('--inpsize', default=768,\n",
    "                   help='embedding size')\n",
    "parser.add_argument('--vocabsize', default=768,\n",
    "                   help='vocab size')\n",
    "parser.add_argument('--lr', default= 0.001, type=float,\n",
    "                    help=\"Learning rate of loss optimization\")\n",
    "\n",
    "'''\n",
    "parser.add_argument('--data_dir', required=True, type=pathlib.Path, \n",
    "                    help='location to dataset files')\n",
    "parser.add_argument('--device', default=torch.device('cpu'), \n",
    "                    help='gpu or cpu')\n",
    "parser.add_argument('--mtype', default='linear', \n",
    "                    help='Type of model')\n",
    "parser.add_argument('--load_model', action='store_true', \n",
    "                    help='To load and run model')\n",
    "parser.add_argument('--save_dir', default='/home/amsinha/wsd-grid/wsd/scripts/runs/', \n",
    "                    help='model saving dir')\n",
    "parser.add_argument('--model_num', default=1, type=str, \n",
    "                    help='saved model identifier')\n",
    "parser.add_argument('--save-model', action='store_true', \n",
    "                    help='to save the model')\n",
    "parser.add_argument('--early-stopping', action='store_true', \n",
    "                    help='to save checkpoint early')\n",
    "parser.add_argument('--report-every', default=10, type=int,\n",
    "                    help='Log report period')\n",
    "\n",
    "parser.add_argument('--patience', default=5, \n",
    "                    help='patience for early_stopping')\n",
    "parser.add_argument('--semantics', action='store_true', \n",
    "                    help='To load semantics A')\n",
    "parser.add_argument('--trainable', action='store_true', help='to train adjancency')\n",
    "parser.add_argument('--fragment', action='store_true')\n",
    "'''\n",
    "\n",
    "params,_ = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn : inp_dim x rnn_hidden_dim x n_layer\n",
    "# input : bs x seq_len x inp_dim\n",
    "# h0 : n_layer x bs x rnn_hidden_dim\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2, batch_first=True)\n",
    "input,h0 = torch.randn(3, 5, 10),torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nermodel(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(nermodel, self).__init__()\n",
    "        self.device = params.device\n",
    "        self.hiddensize = params.hs\n",
    "        self.num_layer = params.nl\n",
    "        self.bidir = params.bidir\n",
    "        self.bs = params.bs\n",
    "        self.inplen = params.inplen\n",
    "        self.inpsize = params.inpsize\n",
    "        self.vocabsize = params.vocabsize\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(self.vocabsize, self.inpsize)\n",
    "        # keep batch first\n",
    "        self.rnn = nn.GRU(params.inpsize, params.hs, \\\n",
    "                          num_layers=self.num_layer, batch_first=True)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(self.bidir * self.hiddensize, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('x_i:',x.shape, 'h0:',self.h0.shape)\n",
    "        self.h0 = torch.randn(self.bidir * self.num_layer, \n",
    "                              x.shape[0], self.hiddensize)\n",
    "        x = self.char_embedding(x)\n",
    "        rnn_otpt, hn = self.rnn(x, self.h0)\n",
    "        #print(rnn_otpt.shape)\n",
    "        fc_otpt = self.relu(self.fc(rnn_otpt))\n",
    "        #print('o shape:',fc_otpt.shape)\n",
    "        return fc_otpt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: nermodel(\n",
      "  (char_embedding): Embedding(805, 768)\n",
      "  (rnn): GRU(768, 768, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "x_i: torch.Size([32, 50]) h0: torch.Size([2, 32, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "o shape: torch.Size([32, 50, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model testing\n",
    "params.vocabsize = 805#len(train_set.vocab)\n",
    "m = nermodel(params)\n",
    "trial_inp = torch.randint(0, 805, (32,50))\n",
    "print('model:', m)\n",
    "m(trial_inp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader and Input preprocessing - char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>span</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525872716411580416</td>\n",
       "      <td>2333890110</td>\n",
       "      <td>2014-10-25</td>\n",
       "      <td>@Rhy_QD10 yeah irking he need his ass whoop I ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>809577207597244417</td>\n",
       "      <td>165916824</td>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>Panda Express üêºüòõ</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>590918768269864960</td>\n",
       "      <td>2414667758</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>Well..technology wins agains. People are fight...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>237385144221192193</td>\n",
       "      <td>24324898</td>\n",
       "      <td>2012-08-20</td>\n",
       "      <td>@jennabennabear what happened?????</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166288274300735488</td>\n",
       "      <td>181819579</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>A first grade teacher asked her class to compl...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id     user_id  created_at  \\\n",
       "0  525872716411580416  2333890110  2014-10-25   \n",
       "1  809577207597244417   165916824  2016-12-16   \n",
       "2  590918768269864960  2414667758  2015-04-22   \n",
       "3  237385144221192193    24324898  2012-08-20   \n",
       "4  166288274300735488   181819579  2012-02-05   \n",
       "\n",
       "                                                text start end span drug  \n",
       "0  @Rhy_QD10 yeah irking he need his ass whoop I ...     -   -    -    -  \n",
       "1                                   Panda Express üêºüòõ     -   -    -    -  \n",
       "2  Well..technology wins agains. People are fight...     -   -    -    -  \n",
       "3                 @jennabennabear what happened?????     -   -    -    -  \n",
       "4  A first grade teacher asked her class to compl...     -   -    -    -  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/BioCreative_TrainTask3.0.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('../data/BioCreative_ValTask3.tsv', sep='\\t')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive\n",
    "tpdf = train_df.loc[train_df['start'] != '-']\n",
    "tndf = train_df.loc[train_df['start'] == '-']\n",
    "dpdf = dev_df.loc[dev_df['start'] != '-']\n",
    "dndf = dev_df.loc[dev_df['start'] == '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>span</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22964</th>\n",
       "      <td>665056375295184896</td>\n",
       "      <td>1000206378</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>anyone have muscle relaxers or vicodeine !??</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>Vicodeine</td>\n",
       "      <td>vicodin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>488852816237039618</td>\n",
       "      <td>2333890110</td>\n",
       "      <td>2014-07-15</td>\n",
       "      <td>@muslimah_fatima bye girl he can't see shit he...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>709112139575795712</td>\n",
       "      <td>250358074</td>\n",
       "      <td>2016-03-13</td>\n",
       "      <td>@fash_chronicles we're dress twins!!üëØ yay! i t...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>549622749896130561</td>\n",
       "      <td>151190725</td>\n",
       "      <td>2014-12-29</td>\n",
       "      <td>@misskd @ashstronge lol so did we! my waist is...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>550132579274620928</td>\n",
       "      <td>2799031971</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>the official nesting has started. i have gotte...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>852222771170095108</td>\n",
       "      <td>1267370436</td>\n",
       "      <td>2017-04-12</td>\n",
       "      <td>@_reynaportillo @siml_25 dude go swimming !! üò©</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19955</th>\n",
       "      <td>407728229857112064</td>\n",
       "      <td>220948289</td>\n",
       "      <td>2013-12-03</td>\n",
       "      <td>the nightly pill arsenal... acetaminophen, 3 d...</td>\n",
       "      <td>28</td>\n",
       "      <td>41</td>\n",
       "      <td>Acetaminophen</td>\n",
       "      <td>acetaminophen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>825077608794689538</td>\n",
       "      <td>2511267649</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>stop posting them. stop tweeting about them. s...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>553397745701376000</td>\n",
       "      <td>2333890110</td>\n",
       "      <td>2015-01-09</td>\n",
       "      <td>@mamas_ripdad thank yu üòò</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>477779269133676544</td>\n",
       "      <td>24564914</td>\n",
       "      <td>2014-06-14</td>\n",
       "      <td>floral bird skull dangles! our big summer sale...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id     user_id  created_at  \\\n",
       "22964  665056375295184896  1000206378  2015-11-13   \n",
       "56     488852816237039618  2333890110  2014-07-15   \n",
       "301    709112139575795712   250358074  2016-03-13   \n",
       "273    549622749896130561   151190725  2014-12-29   \n",
       "342    550132579274620928  2799031971  2014-12-31   \n",
       "...                   ...         ...         ...   \n",
       "157    852222771170095108  1267370436  2017-04-12   \n",
       "19955  407728229857112064   220948289  2013-12-03   \n",
       "305    825077608794689538  2511267649  2017-01-27   \n",
       "2      553397745701376000  2333890110  2015-01-09   \n",
       "88     477779269133676544    24564914  2014-06-14   \n",
       "\n",
       "                                                    text start end  \\\n",
       "22964       anyone have muscle relaxers or vicodeine !??    31  40   \n",
       "56     @muslimah_fatima bye girl he can't see shit he...     -   -   \n",
       "301    @fash_chronicles we're dress twins!!üëØ yay! i t...     -   -   \n",
       "273    @misskd @ashstronge lol so did we! my waist is...     -   -   \n",
       "342    the official nesting has started. i have gotte...     -   -   \n",
       "...                                                  ...   ...  ..   \n",
       "157       @_reynaportillo @siml_25 dude go swimming !! üò©     -   -   \n",
       "19955  the nightly pill arsenal... acetaminophen, 3 d...    28  41   \n",
       "305    stop posting them. stop tweeting about them. s...     -   -   \n",
       "2                               @mamas_ripdad thank yu üòò     -   -   \n",
       "88     floral bird skull dangles! our big summer sale...     -   -   \n",
       "\n",
       "                span           drug  \n",
       "22964      Vicodeine        vicodin  \n",
       "56                 -              -  \n",
       "301                -              -  \n",
       "273                -              -  \n",
       "342                -              -  \n",
       "...              ...            ...  \n",
       "157                -              -  \n",
       "19955  Acetaminophen  acetaminophen  \n",
       "305                -              -  \n",
       "2                  -              -  \n",
       "88                 -              -  \n",
       "\n",
       "[501 rows x 8 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traindf - subsampled tdf\n",
    "tdf = pd.concat([tpdf,tndf.iloc[:877]])\n",
    "tdf = tdf.sample(frac=1)\n",
    "# devdf - subsampled ddf\n",
    "ddf = pd.concat([dpdf,dndf.iloc[:396]])\n",
    "ddf = ddf.sample(frac=1)\n",
    "\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class biodata(Dataset):\n",
    "    def __init__(self, df, vocab=None, name='train'):\n",
    "        self.len = len(df)\n",
    "        self.data = df\n",
    "        self.max_len = max(df.text.apply(lambda x: len(x)).to_numpy())\n",
    "        self.setname = name\n",
    "        self.vocab = vocab\n",
    "        self.vdict = None\n",
    "        self.create_vocab(vocab)\n",
    "        self.data['text'] = self.data['text'].apply(lambda x: x.lower())\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sen = self.data['text'].iloc[index]\n",
    "        start, end = self.data['start'].iloc[index], self.data['end'].iloc[index]\n",
    "        start = 0 if start == '-' else int(start)\n",
    "        end = 0 if end == '-' else int(end)\n",
    "        return {'ids' : torch.tensor(self.transform_input(sen, pad=True), dtype=torch.long),\n",
    "               'targets' : torch.tensor(self.make_label(self.max_len, start, end), dtype=torch.float64)}\n",
    "    \n",
    "    def transform_input(self, sentence, pad=False):\n",
    "        es = []\n",
    "        for e in sentence.lower():\n",
    "            if e in self.vdict:\n",
    "                es.append(self.vdict[e])\n",
    "            else:\n",
    "                es.append(self.vdict['<oov>'])\n",
    "        diff = 0 if self.max_len<len(es) else self.max_len-len(es)\n",
    "        diff = 0 if not pad else diff\n",
    "        return es + [1]*diff\n",
    "    \n",
    "    def make_label(self,l, start, end):\n",
    "        label = np.zeros(l)\n",
    "        try:\n",
    "            if start <= l:\n",
    "                label[start:end] = 1\n",
    "        except:\n",
    "            print('======>',start, l)\n",
    "            import sys;sys.exit()\n",
    "        return label\n",
    "    \n",
    "    def create_vocab(self, vocab):\n",
    "        if not vocab:\n",
    "            iv = {'<oov>', '<pad>'}\n",
    "            for line in self.data['text'].to_numpy():\n",
    "                iv |= set(line)\n",
    "            self.vocab = iv\n",
    "        else:\n",
    "            iv = vocab\n",
    "        ivdict = {'<oov>':0, '<pad>':1}\n",
    "        for e in iv:\n",
    "            if e not in ivdict:\n",
    "                ivdict[e] = len(ivdict)\n",
    "        self.vdict = ivdict\n",
    "        \n",
    "        print(f'{self.setname} vocab created!')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __name__(self):\n",
    "        return self.setname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vocab created!\n",
      "dev vocab created!\n"
     ]
    }
   ],
   "source": [
    "train_set = biodata(tdf, name='train')\n",
    "dev_set = biodata(ddf, vocab=train_set.vocab, name='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': params.bs,\n",
    "               'shuffle':True,\n",
    "               'num_workers': 2}\n",
    "dev_params = {'batch_size': params.bs,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 2}\n",
    "\n",
    "trainloader = DataLoader(train_set, **train_params)\n",
    "devloader = DataLoader(dev_set, **dev_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_steps = 0,0\n",
    "    n_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for _, data in tqdm(enumerate(trainloader)):\n",
    "        optimizer.zero_grad()\n",
    "        ids = data['ids']#.to(device, dtype=torch.long)\n",
    "        tar = data['targets']#.to(device, dtype=torch.long)\n",
    "        output = model(ids).squeeze(-1)\n",
    "        #print('outshape:', output.shape, 'tarshape:', tar.shape)\n",
    "        loss = loss_function(output, tar)\n",
    "        tr_loss += loss.item()\n",
    "        tr_steps += 1\n",
    "        \n",
    "        if _ % 50 == 0:\n",
    "            print(f'training loss per step : {tr_loss/ tr_steps}')\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of nermodel(\n",
       "  (char_embedding): Embedding(805, 768)\n",
       "  (rnn): GRU(768, 768, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per step : 0.733324588171955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:13,  1.65s/it]"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "#torch.nn.CrossEntropyLoss()\n",
    "model = nermodel(params)\n",
    "\n",
    "optimizer = torch.optim.Adam(params =model.parameters(), lr=params.lr)\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    train(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
