{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "#rnncharmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description=\"Running ner...\")\n",
    "parser.add_argument('--device', default = torch.device('cpu'), \n",
    "                    help='cpu or gpu')\n",
    "parser.add_argument('--hs', default=768, type=int,\n",
    "                    help='Hidden layer size')\n",
    "parser.add_argument('--bs', default=32, \n",
    "                    help='batch size')\n",
    "parser.add_argument('--nl', default=2, \n",
    "                    help='Number of layers')\n",
    "parser.add_argument('--bidir', default=1,\n",
    "                   help='bi directional')\n",
    "parser.add_argument('--inplen', default=50,\n",
    "                   help='sequence/sentence length')\n",
    "parser.add_argument('--inpsize', default=768,\n",
    "                   help='embedding size')\n",
    "parser.add_argument('--vocabsize', default=768,\n",
    "                   help='vocab size')\n",
    "parser.add_argument('--lr', default= 0.001, type=float,\n",
    "                    help=\"Learning rate of loss optimization\")\n",
    "\n",
    "'''\n",
    "parser.add_argument('--data_dir', required=True, type=pathlib.Path, \n",
    "                    help='location to dataset files')\n",
    "parser.add_argument('--device', default=torch.device('cpu'), \n",
    "                    help='gpu or cpu')\n",
    "parser.add_argument('--mtype', default='linear', \n",
    "                    help='Type of model')\n",
    "parser.add_argument('--load_model', action='store_true', \n",
    "                    help='To load and run model')\n",
    "parser.add_argument('--save_dir', default='/home/amsinha/wsd-grid/wsd/scripts/runs/', \n",
    "                    help='model saving dir')\n",
    "parser.add_argument('--model_num', default=1, type=str, \n",
    "                    help='saved model identifier')\n",
    "parser.add_argument('--save-model', action='store_true', \n",
    "                    help='to save the model')\n",
    "parser.add_argument('--early-stopping', action='store_true', \n",
    "                    help='to save checkpoint early')\n",
    "parser.add_argument('--report-every', default=10, type=int,\n",
    "                    help='Log report period')\n",
    "\n",
    "parser.add_argument('--patience', default=5, \n",
    "                    help='patience for early_stopping')\n",
    "parser.add_argument('--semantics', action='store_true', \n",
    "                    help='To load semantics A')\n",
    "parser.add_argument('--trainable', action='store_true', help='to train adjancency')\n",
    "parser.add_argument('--fragment', action='store_true')\n",
    "'''\n",
    "\n",
    "params,_ = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn : inp_dim x rnn_hidden_dim x n_layer\n",
    "# input : bs x seq_len x inp_dim\n",
    "# h0 : n_layer x bs x rnn_hidden_dim\n",
    "\n",
    "rnn = nn.GRU(10, 20, 2, batch_first=True)\n",
    "input,h0 = torch.randn(3, 5, 10),torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nermodel(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(nermodel, self).__init__()\n",
    "        self.device = params.device\n",
    "        self.hiddensize = params.hs\n",
    "        self.num_layer = params.nl\n",
    "        self.bidir = params.bidir\n",
    "        self.bs = params.bs\n",
    "        self.inplen = params.inplen\n",
    "        self.inpsize = params.inpsize\n",
    "        self.vocabsize = params.vocabsize\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(self.vocabsize, self.inpsize)\n",
    "        # keep batch first\n",
    "        self.rnn = nn.GRU(params.inpsize, params.hs, \\\n",
    "                          num_layers=self.num_layer, batch_first=True)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(self.bidir * self.hiddensize, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('x_i:',x.shape, 'h0:',self.h0.shape)\n",
    "        self.h0 = torch.randn(self.bidir * self.num_layer, \n",
    "                              x.shape[0], self.hiddensize)\n",
    "        x = self.char_embedding(x)\n",
    "        rnn_otpt, hn = self.rnn(x, self.h0)\n",
    "        #print(rnn_otpt.shape)\n",
    "        fc_otpt = self.relu(self.fc(rnn_otpt))\n",
    "        #print('o shape:',fc_otpt.shape)\n",
    "        return fc_otpt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: nermodel(\n",
      "  (char_embedding): Embedding(805, 768)\n",
      "  (rnn): GRU(768, 768, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model testing\n",
    "params.vocabsize = 805#len(train_set.vocab)\n",
    "m = nermodel(params)\n",
    "trial_inp = torch.randint(0, 805, (32,50))\n",
    "print('model:', m)\n",
    "m(trial_inp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader and Input preprocessing - char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>span</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525872716411580416</td>\n",
       "      <td>2333890110</td>\n",
       "      <td>2014-10-25</td>\n",
       "      <td>@Rhy_QD10 yeah irking he need his ass whoop I ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>809577207597244417</td>\n",
       "      <td>165916824</td>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>Panda Express üêºüòõ</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>590918768269864960</td>\n",
       "      <td>2414667758</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>Well..technology wins agains. People are fight...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>237385144221192193</td>\n",
       "      <td>24324898</td>\n",
       "      <td>2012-08-20</td>\n",
       "      <td>@jennabennabear what happened?????</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166288274300735488</td>\n",
       "      <td>181819579</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>A first grade teacher asked her class to compl...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>760235205047320577</td>\n",
       "      <td>1143892999</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>Damnnnnnnnnnn I didn't know it cost 20 to get ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>860946084117577728</td>\n",
       "      <td>583610519</td>\n",
       "      <td>2017-05-06</td>\n",
       "      <td>I got good news today! @sieelyn_ &amp;amp; Austin ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>798113426644041729</td>\n",
       "      <td>21343364</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>@MissSarahLou6 @CocaCola_GB what?? I never got...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>790015640174080000</td>\n",
       "      <td>65087044</td>\n",
       "      <td>2016-10-23</td>\n",
       "      <td>Every last episode was funny as fuck lol</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>416993821474713600</td>\n",
       "      <td>1025811572</td>\n",
       "      <td>2013-12-28</td>\n",
       "      <td>@josiestolla now i don't do drama. bounce.</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id     user_id  created_at  \\\n",
       "0      525872716411580416  2333890110  2014-10-25   \n",
       "1      809577207597244417   165916824  2016-12-16   \n",
       "2      590918768269864960  2414667758  2015-04-22   \n",
       "3      237385144221192193    24324898  2012-08-20   \n",
       "4      166288274300735488   181819579  2012-02-05   \n",
       "...                   ...         ...         ...   \n",
       "49995  760235205047320577  1143892999  2016-08-01   \n",
       "49996  860946084117577728   583610519  2017-05-06   \n",
       "49997  798113426644041729    21343364  2016-11-14   \n",
       "49998  790015640174080000    65087044  2016-10-23   \n",
       "49999  416993821474713600  1025811572  2013-12-28   \n",
       "\n",
       "                                                    text start end span drug  \n",
       "0      @Rhy_QD10 yeah irking he need his ass whoop I ...     -   -    -    -  \n",
       "1                                       Panda Express üêºüòõ     -   -    -    -  \n",
       "2      Well..technology wins agains. People are fight...     -   -    -    -  \n",
       "3                     @jennabennabear what happened?????     -   -    -    -  \n",
       "4      A first grade teacher asked her class to compl...     -   -    -    -  \n",
       "...                                                  ...   ...  ..  ...  ...  \n",
       "49995  Damnnnnnnnnnn I didn't know it cost 20 to get ...     -   -    -    -  \n",
       "49996  I got good news today! @sieelyn_ &amp; Austin ...     -   -    -    -  \n",
       "49997  @MissSarahLou6 @CocaCola_GB what?? I never got...     -   -    -    -  \n",
       "49998           Every last episode was funny as fuck lol     -   -    -    -  \n",
       "49999         @josiestolla now i don't do drama. bounce.     -   -    -    -  \n",
       "\n",
       "[50000 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/BioCreative_TrainTask3.0.tsv', sep='\\t')\n",
    "#train_df = pd.read_csv('../data/SMM4H18_train_modified.csv', sep='\\t')\n",
    "dev_df = pd.read_csv('../data/BioCreative_ValTask3.tsv', sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive\n",
    "tpdf = train_df.loc[train_df['start'] != '-']\n",
    "tndf = train_df.loc[train_df['start'] == '-']\n",
    "dpdf = dev_df.loc[dev_df['start'] != '-']\n",
    "dndf = dev_df.loc[dev_df['start'] == '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>span</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>707715042477744128</td>\n",
       "      <td>218297421</td>\n",
       "      <td>2016-03-09</td>\n",
       "      <td>Just get on birth control and use two condoms....</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>birth control</td>\n",
       "      <td>birth control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>841466126655664128</td>\n",
       "      <td>184620477</td>\n",
       "      <td>2017-03-14</td>\n",
       "      <td>I have a possible infection from my Zofran pum...</td>\n",
       "      <td>36</td>\n",
       "      <td>42</td>\n",
       "      <td>Zofran</td>\n",
       "      <td>zofran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>612522575776907264</td>\n",
       "      <td>2206562811</td>\n",
       "      <td>2015-06-21</td>\n",
       "      <td>@bethanygiuffre it's time for the epidural!</td>\n",
       "      <td>34</td>\n",
       "      <td>42</td>\n",
       "      <td>epidural</td>\n",
       "      <td>epidural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>767004439450451968</td>\n",
       "      <td>19546372</td>\n",
       "      <td>2016-08-20</td>\n",
       "      <td>@elvisrockysly yeah, one is heparin. Jesus, th...</td>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "      <td>heparin</td>\n",
       "      <td>heparin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>765379008838131712</td>\n",
       "      <td>1486889246</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>*puts cocoa butter on, takes prenatal vitamin,...</td>\n",
       "      <td>29</td>\n",
       "      <td>45</td>\n",
       "      <td>prenatal vitamin</td>\n",
       "      <td>prenatal vitamins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>629018857571614720</td>\n",
       "      <td>2557491804</td>\n",
       "      <td>2015-08-05</td>\n",
       "      <td>@AlexisBourque1 @jiannabroussard @Love_Nylaaaa...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>644596144681697281</td>\n",
       "      <td>2783087789</td>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>almost 19 with no h.s. diploma or job about to...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>769520550012186624</td>\n",
       "      <td>252235856</td>\n",
       "      <td>2016-08-27</td>\n",
       "      <td>I loveee bananas üòã</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>226415059742638080</td>\n",
       "      <td>24324898</td>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>Just posted a photo http://t.co/fFN4aShq</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>487319789330001920</td>\n",
       "      <td>2309387798</td>\n",
       "      <td>2014-07-10</td>\n",
       "      <td>‚Äú@LK_Countin: @__rastaaaaa congrats on the sho...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id     user_id  created_at  \\\n",
       "234   707715042477744128   218297421  2016-03-09   \n",
       "550   841466126655664128   184620477  2017-03-14   \n",
       "921   612522575776907264  2206562811  2015-06-21   \n",
       "968   767004439450451968    19546372  2016-08-20   \n",
       "1410  765379008838131712  1486889246  2016-08-16   \n",
       "...                  ...         ...         ...   \n",
       "392   629018857571614720  2557491804  2015-08-05   \n",
       "393   644596144681697281  2783087789  2015-09-17   \n",
       "394   769520550012186624   252235856  2016-08-27   \n",
       "395   226415059742638080    24324898  2012-07-20   \n",
       "396   487319789330001920  2309387798  2014-07-10   \n",
       "\n",
       "                                                   text start end  \\\n",
       "234   Just get on birth control and use two condoms....    12  25   \n",
       "550   I have a possible infection from my Zofran pum...    36  42   \n",
       "921         @bethanygiuffre it's time for the epidural!    34  42   \n",
       "968   @elvisrockysly yeah, one is heparin. Jesus, th...    28  35   \n",
       "1410  *puts cocoa butter on, takes prenatal vitamin,...    29  45   \n",
       "...                                                 ...   ...  ..   \n",
       "392   @AlexisBourque1 @jiannabroussard @Love_Nylaaaa...     -   -   \n",
       "393   almost 19 with no h.s. diploma or job about to...     -   -   \n",
       "394                                  I loveee bananas üòã     -   -   \n",
       "395            Just posted a photo http://t.co/fFN4aShq     -   -   \n",
       "396   ‚Äú@LK_Countin: @__rastaaaaa congrats on the sho...     -   -   \n",
       "\n",
       "                  span               drug  \n",
       "234      birth control      birth control  \n",
       "550             Zofran             zofran  \n",
       "921           epidural           epidural  \n",
       "968            heparin            heparin  \n",
       "1410  prenatal vitamin  prenatal vitamins  \n",
       "...                ...                ...  \n",
       "392                  -                  -  \n",
       "393                  -                  -  \n",
       "394                  -                  -  \n",
       "395                  -                  -  \n",
       "396                  -                  -  \n",
       "\n",
       "[501 rows x 8 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traindf - subsampled tdf\n",
    "tdf = pd.concat([tpdf,tndf.iloc[:877]])\n",
    "tdf = tdf.sample(frac=1)\n",
    "# devdf - subsampled ddf\n",
    "ddf = pd.concat([dpdf,dndf.iloc[:396]])\n",
    "#ddf = ddf.sample(frac=1)\n",
    "\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class biodata(Dataset):\n",
    "    def __init__(self, df, vocab=None, name='train'):\n",
    "        self.len = len(df)\n",
    "        self.data = df\n",
    "        self.max_len = max(df.text.apply(lambda x: len(x)).to_numpy())\n",
    "        self.setname = name\n",
    "        self.vocab = vocab\n",
    "        self.vdict = None\n",
    "        self.create_vocab(vocab)\n",
    "        self.data['text'] = self.data['text'].apply(lambda x: x.lower())\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sen = self.data['text'].iloc[index]\n",
    "        start, end = self.data['start'].iloc[index], self.data['end'].iloc[index]\n",
    "        start = 0 if start == '-' else int(start)\n",
    "        end = 0 if end == '-' else int(end)\n",
    "        return {'ids' : torch.tensor(self.transform_input(sen, pad=True), dtype=torch.long),\n",
    "               'targets' : torch.tensor(self.make_label(self.max_len, start, end), dtype=torch.float64)}\n",
    "    \n",
    "    def transform_input(self, sentence, pad=False):\n",
    "        es = []\n",
    "        for e in sentence.lower():\n",
    "            if e in self.vdict:\n",
    "                es.append(self.vdict[e])\n",
    "            else:\n",
    "                es.append(self.vdict['<oov>'])\n",
    "        diff = 0 if self.max_len<len(es) else self.max_len-len(es)\n",
    "        diff = 0 if not pad else diff\n",
    "        return es + [1]*diff\n",
    "    \n",
    "    def make_label(self,l, start, end):\n",
    "        label = np.zeros(l)\n",
    "        try:\n",
    "            if start <= l:\n",
    "                label[start:end] = 1\n",
    "        except:\n",
    "            print('======>',start, l)\n",
    "            import sys;sys.exit()\n",
    "        return label\n",
    "    \n",
    "    def create_vocab(self, vocab):\n",
    "        if not vocab:\n",
    "            iv = {'<oov>', '<pad>'}\n",
    "            for line in self.data['text'].to_numpy():\n",
    "                iv |= set(line)\n",
    "            self.vocab = iv\n",
    "        else:\n",
    "            iv = vocab\n",
    "        ivdict = {'<oov>':0, '<pad>':1}\n",
    "        for e in iv:\n",
    "            if e not in ivdict:\n",
    "                ivdict[e] = len(ivdict)\n",
    "        self.vdict = ivdict\n",
    "        \n",
    "        print(f'{self.setname} vocab created!')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __name__(self):\n",
    "        return self.setname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vocab created!\n",
      "dev vocab created!\n"
     ]
    }
   ],
   "source": [
    "train_set = biodata(tdf, name='train')\n",
    "dev_set = biodata(ddf, vocab=train_set.vocab, name='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': params.bs,\n",
    "               'shuffle':True,\n",
    "               'num_workers': 2}\n",
    "dev_params = {'batch_size': params.bs,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 2}\n",
    "\n",
    "trainloader = DataLoader(train_set, **train_params)\n",
    "devloader = DataLoader(dev_set, **dev_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_steps = 0,0\n",
    "    n_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    for _, data in tqdm(enumerate(trainloader)):\n",
    "        optimizer.zero_grad()\n",
    "        ids = data['ids']#.to(device, dtype=torch.long)\n",
    "        tar = data['targets']#.to(device, dtype=torch.long)\n",
    "        output = model(ids).squeeze(-1)\n",
    "        loss = loss_function(output, tar)\n",
    "        tr_loss += loss.item()\n",
    "        tr_steps += 1\n",
    "        \n",
    "        #acc_light = \n",
    "\n",
    "        if _ % 50 == 0:\n",
    "            print(f'training loss per 50 step : {tr_loss/ tr_steps}')\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def eval(testloader):\n",
    "    test_loss, test_steps = 0,0\n",
    "    n_correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testloader)):\n",
    "            \n",
    "            ids = data['ids']#.to(device, dtype=torch.long)\n",
    "            tar = data['targets']#.to(device, dtype=torch.long)\n",
    "            output = model(ids).squeeze(-1)\n",
    "            loss = loss_function(output, tar)\n",
    "            test_loss += loss.item()\n",
    "            test_steps += 1\n",
    "\n",
    "            if _ % 50 == 0:\n",
    "                print(f'Testing loss per 50 step : {test_loss/ test_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of nermodel(\n",
       "  (char_embedding): Embedding(805, 768)\n",
       "  (rnn): GRU(768, 768, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.7210162563664788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [01:40,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6937965346892583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [03:36,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6934781471940069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [05:32,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6933707815770979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [07:40,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6933163647809388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [08:25,  2.30s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6931471824645996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [02:13,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6931498325339908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [04:23,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6931491550368587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [06:42,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6931502173916612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [09:01,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss per 50 step : 0.6931489466073624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [09:56,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss() #torch.nn.CrossEntropyLoss()\n",
    "model = nermodel(params)\n",
    "optimizer = torch.optim.Adam(params =model.parameters(), lr=params.lr)\n",
    "EPOCHS = 2\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    train(e)\n",
    "   # if e:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:08,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "## evaluating labels\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for _, data in tqdm(enumerate(devloader)):\n",
    "        #optimizer.zero_grad()\n",
    "        ids = data['ids']#.to(device, dtype=torch.long)\n",
    "        tar = data['targets']#.to(device, dtype=torch.long)\n",
    "        output = model(ids).squeeze(-1)\n",
    "        outputs.append(output)\n",
    "    #loss = loss_function(output, tar)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7015, dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(output, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for t,o in zip(tar, output):\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nonzero or threshold\n",
    "# consider all comparison for correct with 1s vs compare with all\n",
    "# loss consider all\n",
    "sum((output > 0).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>span</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>707715042477744128</td>\n",
       "      <td>218297421</td>\n",
       "      <td>2016-03-09</td>\n",
       "      <td>just get on birth control and use two condoms....</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841466126655664128</td>\n",
       "      <td>184620477</td>\n",
       "      <td>2017-03-14</td>\n",
       "      <td>i have a possible infection from my zofran pum...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>612522575776907264</td>\n",
       "      <td>2206562811</td>\n",
       "      <td>2015-06-21</td>\n",
       "      <td>@bethanygiuffre it's time for the epidural!</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>767004439450451968</td>\n",
       "      <td>19546372</td>\n",
       "      <td>2016-08-20</td>\n",
       "      <td>@elvisrockysly yeah, one is heparin. jesus, th...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>765379008838131712</td>\n",
       "      <td>1486889246</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>*puts cocoa butter on, takes prenatal vitamin,...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>448063246231015424</td>\n",
       "      <td>113610499</td>\n",
       "      <td>2014-03-24</td>\n",
       "      <td>update: i got the epidural, back contractions ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>448063246231015424</td>\n",
       "      <td>113610499</td>\n",
       "      <td>2014-03-24</td>\n",
       "      <td>update: i got the epidural, back contractions ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>502162994231791617</td>\n",
       "      <td>17084008</td>\n",
       "      <td>2014-08-20</td>\n",
       "      <td>i seem to have to wash most cups of tea down w...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>534528525379055617</td>\n",
       "      <td>2551446475</td>\n",
       "      <td>2014-11-18</td>\n",
       "      <td>our kid is now 25 weeks and the size of a turn...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>786755084231270401</td>\n",
       "      <td>177251944</td>\n",
       "      <td>2016-10-14</td>\n",
       "      <td>still have one day of antibiotics left for my ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>692192378933923841</td>\n",
       "      <td>1959451166</td>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>@pugzznotdrugz i've heard the depo can be toxi...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>557780893323231232</td>\n",
       "      <td>2799031971</td>\n",
       "      <td>2015-01-21</td>\n",
       "      <td>muscle relaxers üëå</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>486973488775700481</td>\n",
       "      <td>135289182</td>\n",
       "      <td>2014-07-09</td>\n",
       "      <td>popping tums like candy</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>837736300190945280</td>\n",
       "      <td>184620477</td>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>i can't decide if my migraine is from me being...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>467374169038942208</td>\n",
       "      <td>23474083</td>\n",
       "      <td>2014-05-16</td>\n",
       "      <td>has anyone tried making a crushed asprin + wat...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>300047510033031168</td>\n",
       "      <td>24324898</td>\n",
       "      <td>2013-02-09</td>\n",
       "      <td>@jennabennabear also i'm gonna buy liquid vita...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>804468435656839168</td>\n",
       "      <td>177251944</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>@grneyedgirl1970 üòî the antibiotics should star...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>804468435656839168</td>\n",
       "      <td>177251944</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>@grneyedgirl1970 üòî the antibiotics should star...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>328544164151185408</td>\n",
       "      <td>18660643</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>ivf update - first injection of menopur and fo...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>328544164151185408</td>\n",
       "      <td>18660643</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>ivf update - first injection of menopur and fo...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>436463012325888000</td>\n",
       "      <td>20043090</td>\n",
       "      <td>2014-02-20</td>\n",
       "      <td>how many syns in a lemsip? feeling ruff :-(</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>447993192483287040</td>\n",
       "      <td>113610499</td>\n",
       "      <td>2014-03-24</td>\n",
       "      <td>fentanyl is where it's at!!!! üëãgoodbye pain, p...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>529668542036447232</td>\n",
       "      <td>2444385272</td>\n",
       "      <td>2014-11-04</td>\n",
       "      <td>too bad i'm terrified of the epidural</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>557235045044203523</td>\n",
       "      <td>2799031971</td>\n",
       "      <td>2015-01-19</td>\n",
       "      <td>update: epidural was really rough but worth it...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>521253215015534592</td>\n",
       "      <td>2444385272</td>\n",
       "      <td>2014-10-12</td>\n",
       "      <td>i'm terrified of labor especially since i'm ha...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>724795259977383936</td>\n",
       "      <td>1959451166</td>\n",
       "      <td>2016-04-26</td>\n",
       "      <td>can't see bc @anthony_drakee put earache drops...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>724795259977383936</td>\n",
       "      <td>1959451166</td>\n",
       "      <td>2016-04-26</td>\n",
       "      <td>can't see bc @anthony_drakee put earache drops...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>598062290970095617</td>\n",
       "      <td>2546638931</td>\n",
       "      <td>2015-05-12</td>\n",
       "      <td>' when your up on a 4 am mission to walmart fo...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>604499011752501248</td>\n",
       "      <td>23707743</td>\n",
       "      <td>2015-05-30</td>\n",
       "      <td>@chelsea464 benadryl!</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>758762348144369664</td>\n",
       "      <td>19546372</td>\n",
       "      <td>2016-07-28</td>\n",
       "      <td>saira to renee: you look scared.   that would ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>501830595610415105</td>\n",
       "      <td>17084008</td>\n",
       "      <td>2014-08-19</td>\n",
       "      <td>@viccimcwill am guessing you had an epidural ;...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>501830595610415105</td>\n",
       "      <td>17084008</td>\n",
       "      <td>2014-08-19</td>\n",
       "      <td>@viccimcwill am guessing you had an epidural ;...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id     user_id  created_at  \\\n",
       "0   707715042477744128   218297421  2016-03-09   \n",
       "1   841466126655664128   184620477  2017-03-14   \n",
       "2   612522575776907264  2206562811  2015-06-21   \n",
       "3   767004439450451968    19546372  2016-08-20   \n",
       "4   765379008838131712  1486889246  2016-08-16   \n",
       "5   448063246231015424   113610499  2014-03-24   \n",
       "6   448063246231015424   113610499  2014-03-24   \n",
       "7   502162994231791617    17084008  2014-08-20   \n",
       "8   534528525379055617  2551446475  2014-11-18   \n",
       "9   786755084231270401   177251944  2016-10-14   \n",
       "10  692192378933923841  1959451166  2016-01-27   \n",
       "11  557780893323231232  2799031971  2015-01-21   \n",
       "12  486973488775700481   135289182  2014-07-09   \n",
       "13  837736300190945280   184620477  2017-03-03   \n",
       "14  467374169038942208    23474083  2014-05-16   \n",
       "15  300047510033031168    24324898  2013-02-09   \n",
       "16  804468435656839168   177251944  2016-12-01   \n",
       "17  804468435656839168   177251944  2016-12-01   \n",
       "18  328544164151185408    18660643  2013-04-28   \n",
       "19  328544164151185408    18660643  2013-04-28   \n",
       "20  436463012325888000    20043090  2014-02-20   \n",
       "21  447993192483287040   113610499  2014-03-24   \n",
       "22  529668542036447232  2444385272  2014-11-04   \n",
       "23  557235045044203523  2799031971  2015-01-19   \n",
       "24  521253215015534592  2444385272  2014-10-12   \n",
       "25  724795259977383936  1959451166  2016-04-26   \n",
       "26  724795259977383936  1959451166  2016-04-26   \n",
       "27  598062290970095617  2546638931  2015-05-12   \n",
       "28  604499011752501248    23707743  2015-05-30   \n",
       "29  758762348144369664    19546372  2016-07-28   \n",
       "30  501830595610415105    17084008  2014-08-19   \n",
       "31  501830595610415105    17084008  2014-08-19   \n",
       "\n",
       "                                                 text start end span drug  \n",
       "0   just get on birth control and use two condoms....     -   -    -    -  \n",
       "1   i have a possible infection from my zofran pum...     -   -    -    -  \n",
       "2         @bethanygiuffre it's time for the epidural!     -   -    -    -  \n",
       "3   @elvisrockysly yeah, one is heparin. jesus, th...     -   -    -    -  \n",
       "4   *puts cocoa butter on, takes prenatal vitamin,...     -   -    -    -  \n",
       "5   update: i got the epidural, back contractions ...     -   -    -    -  \n",
       "6   update: i got the epidural, back contractions ...     -   -    -    -  \n",
       "7   i seem to have to wash most cups of tea down w...     -   -    -    -  \n",
       "8   our kid is now 25 weeks and the size of a turn...     -   -    -    -  \n",
       "9   still have one day of antibiotics left for my ...     -   -    -    -  \n",
       "10  @pugzznotdrugz i've heard the depo can be toxi...     -   -    -    -  \n",
       "11                                  muscle relaxers üëå     -   -    -    -  \n",
       "12                            popping tums like candy     -   -    -    -  \n",
       "13  i can't decide if my migraine is from me being...     -   -    -    -  \n",
       "14  has anyone tried making a crushed asprin + wat...     -   -    -    -  \n",
       "15  @jennabennabear also i'm gonna buy liquid vita...     -   -    -    -  \n",
       "16  @grneyedgirl1970 üòî the antibiotics should star...     -   -    -    -  \n",
       "17  @grneyedgirl1970 üòî the antibiotics should star...     -   -    -    -  \n",
       "18  ivf update - first injection of menopur and fo...     -   -    -    -  \n",
       "19  ivf update - first injection of menopur and fo...     -   -    -    -  \n",
       "20        how many syns in a lemsip? feeling ruff :-(     -   -    -    -  \n",
       "21  fentanyl is where it's at!!!! üëãgoodbye pain, p...     -   -    -    -  \n",
       "22              too bad i'm terrified of the epidural     -   -    -    -  \n",
       "23  update: epidural was really rough but worth it...     -   -    -    -  \n",
       "24  i'm terrified of labor especially since i'm ha...     -   -    -    -  \n",
       "25  can't see bc @anthony_drakee put earache drops...     -   -    -    -  \n",
       "26  can't see bc @anthony_drakee put earache drops...     -   -    -    -  \n",
       "27  ' when your up on a 4 am mission to walmart fo...     -   -    -    -  \n",
       "28                              @chelsea464 benadryl!     -   -    -    -  \n",
       "29  saira to renee: you look scared.   that would ...     -   -    -    -  \n",
       "30  @viccimcwill am guessing you had an epidural ;...     -   -    -    -  \n",
       "31  @viccimcwill am guessing you had an epidural ;...     -   -    -    -  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval post processing\n",
    "#ddf : evaluation data\n",
    "\n",
    "predques = ddf.iloc[:32, 0:4].copy()\n",
    "spans = []\n",
    "for t in output:\n",
    "    #print(t)\n",
    "    span = []\n",
    "    start,end = -1,-1\n",
    "    for i,tt in enumerate(t):\n",
    "        if start == -1 and tt == 1:\n",
    "            start,end = i,i\n",
    "        if start != -1:\n",
    "            if tt == 1:\n",
    "                end +=1\n",
    "            else:\n",
    "                span.append((start,end))\n",
    "                start, end = -1,-1\n",
    "    spans.append(span)\n",
    "print(spans)\n",
    "\n",
    "########################################\n",
    "rest_columens = []\n",
    "for i,sp in enumerate(spans):\n",
    "    if len(sp) == 0:\n",
    "        rest_columens.append(('-','-','-','-'))\n",
    "    else:\n",
    "        # one span detected else first*\n",
    "        if len(sp) == 1:\n",
    "            #print(sp)\n",
    "            s = ddf.iloc[i]['text']\n",
    "            wrd = s[sp[0][0]:sp[0][1]]\n",
    "            rest_columens.append((*(sp[0]),wrd, wrd.lower()))\n",
    "        \n",
    "predans = pd.DataFrame(rest_columens, columns=['start', 'end', 'span', 'drug'])      \n",
    "\n",
    "pred = pd.concat([predques.reset_index(drop=True), predans.reset_index(drop=True)], axis = 1, )\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
