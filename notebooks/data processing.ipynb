{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../twitter_nlp/data/annotated/wnut16/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dev_2015',\n",
       " 'striptypes.py',\n",
       " 'glove.840B.300d.wnut_filtered.txt',\n",
       " 'test_notypes',\n",
       " 'train_notypes',\n",
       " 'dev_2015_notypes',\n",
       " 'test',\n",
       " 'dev_notypes',\n",
       " 'glove_wnut_filtered.200d.txt',\n",
       " 'train',\n",
       " 'dev']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3850\n"
     ]
    }
   ],
   "source": [
    "\" to create json out of original data\"\n",
    "\n",
    "filename = 'test'\n",
    "count = 0\n",
    "sentences = []\n",
    "sentence, ids = [], []\n",
    "with open(data_dir + filename, 'r') as fp:\n",
    "    for line in fp:\n",
    "        if line != '\\n':\n",
    "            info = line.strip().split('\\t')\n",
    "            sentence.append(info[0])\n",
    "            ids.append(info[1])\n",
    "        else:\n",
    "            sentences.append({'idx': count,\n",
    "                              'example': sentence,\n",
    "                             'label':ids})\n",
    "            sentence,ids = [],[]\n",
    "            count += 1\n",
    "print(count)\n",
    "\n",
    "with open(f'../{filename}.json', 'w') as fout:\n",
    "    json.dump(sentences , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "filename = 'train'\n",
    "with open(f'../{filename}.json', \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "labeldict = dict()\n",
    "for d in data:\n",
    "    \n",
    "    for l in d['label']:\n",
    "        if l not in labeldict:\n",
    "            labeldict[l] = 1\n",
    "        else:\n",
    "            labeldict[l] += 1\n",
    "print(len(labeldict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Distribution\n",
    "## 21 classes\n",
    "\n",
    "```\n",
    "train : {'O': 44007, 'B-geo-loc': 276, 'B-facility': 104, 'I-facility': 105, 'B-movie': 34, 'I-movie': 46, 'B-company': 171, 'B-product': 97, 'B-person': 449, 'B-other': 225, 'I-other': 320, 'B-sportsteam': 51, 'I-sportsteam': 23, 'I-product': 80, 'I-company': 36, 'I-person': 215, 'I-geo-loc': 49, 'B-tvshow': 34, 'B-musicartist': 55, 'I-musicartist': 61, 'I-tvshow': 31}\n",
    "\n",
    "dev : {'O': 15133, 'B-other': 132, 'I-other': 97, 'B-geo-loc': 116, 'I-geo-loc': 42, 'B-product': 37, 'I-product': 121, 'B-facility': 38, 'I-facility': 39, 'B-company': 39, 'I-company': 10, 'B-person': 171, 'I-person': 95, 'B-sportsteam': 70, 'B-musicartist': 41, 'I-musicartist': 35, 'I-sportsteam': 13, 'B-movie': 15, 'I-movie': 15, 'B-tvshow': 2}\n",
    "\n",
    "test : {'B-other': 584, 'I-other': 556, 'O': 55953, 'B-movie': 34, 'B-person': 482, 'I-person': 300, 'B-geo-loc': 882, 'B-company': 621, 'I-company': 265, 'B-product': 246, 'I-product': 500, 'B-musicartist': 191, 'I-musicartist': 140, 'B-sportsteam': 147, 'B-facility': 253, 'I-facility': 366, 'I-geo-loc': 219, 'I-movie': 48, 'I-sportsteam': 48, 'B-tvshow': 33, 'I-tvshow': 40}     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://github.com/aritter/twitter_nlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data analysis\n",
    "\n",
    "# task 3 - automatic extraction of medical names from tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|data_split|nexamples|postives|\n",
    "|--|--|--|\n",
    "|train|50k|123|\n",
    "|SMM4H18_train|~10k|4975|\n",
    "|dev|~39k|105|\n",
    "|test|?|?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amansinha/bio-challenge/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>span</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525872716411580416</td>\n",
       "      <td>2333890110</td>\n",
       "      <td>2014-10-25</td>\n",
       "      <td>@Rhy_QD10 yeah irking he need his ass whoop I ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>809577207597244417</td>\n",
       "      <td>165916824</td>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>Panda Express üêºüòõ</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>590918768269864960</td>\n",
       "      <td>2414667758</td>\n",
       "      <td>2015-04-22</td>\n",
       "      <td>Well..technology wins agains. People are fight...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>237385144221192193</td>\n",
       "      <td>24324898</td>\n",
       "      <td>2012-08-20</td>\n",
       "      <td>@jennabennabear what happened?????</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166288274300735488</td>\n",
       "      <td>181819579</td>\n",
       "      <td>2012-02-05</td>\n",
       "      <td>A first grade teacher asked her class to compl...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>760235205047320577</td>\n",
       "      <td>1143892999</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>Damnnnnnnnnnn I didn't know it cost 20 to get ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>860946084117577728</td>\n",
       "      <td>583610519</td>\n",
       "      <td>2017-05-06</td>\n",
       "      <td>I got good news today! @sieelyn_ &amp;amp; Austin ...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>798113426644041729</td>\n",
       "      <td>21343364</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>@MissSarahLou6 @CocaCola_GB what?? I never got...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>790015640174080000</td>\n",
       "      <td>65087044</td>\n",
       "      <td>2016-10-23</td>\n",
       "      <td>Every last episode was funny as fuck lol</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>416993821474713600</td>\n",
       "      <td>1025811572</td>\n",
       "      <td>2013-12-28</td>\n",
       "      <td>@josiestolla now i don't do drama. bounce.</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id     user_id  created_at  \\\n",
       "0      525872716411580416  2333890110  2014-10-25   \n",
       "1      809577207597244417   165916824  2016-12-16   \n",
       "2      590918768269864960  2414667758  2015-04-22   \n",
       "3      237385144221192193    24324898  2012-08-20   \n",
       "4      166288274300735488   181819579  2012-02-05   \n",
       "...                   ...         ...         ...   \n",
       "49995  760235205047320577  1143892999  2016-08-01   \n",
       "49996  860946084117577728   583610519  2017-05-06   \n",
       "49997  798113426644041729    21343364  2016-11-14   \n",
       "49998  790015640174080000    65087044  2016-10-23   \n",
       "49999  416993821474713600  1025811572  2013-12-28   \n",
       "\n",
       "                                                    text start end span drug  \n",
       "0      @Rhy_QD10 yeah irking he need his ass whoop I ...     -   -    -    -  \n",
       "1                                       Panda Express üêºüòõ     -   -    -    -  \n",
       "2      Well..technology wins agains. People are fight...     -   -    -    -  \n",
       "3                     @jennabennabear what happened?????     -   -    -    -  \n",
       "4      A first grade teacher asked her class to compl...     -   -    -    -  \n",
       "...                                                  ...   ...  ..  ...  ...  \n",
       "49995  Damnnnnnnnnnn I didn't know it cost 20 to get ...     -   -    -    -  \n",
       "49996  I got good news today! @sieelyn_ &amp; Austin ...     -   -    -    -  \n",
       "49997  @MissSarahLou6 @CocaCola_GB what?? I never got...     -   -    -    -  \n",
       "49998           Every last episode was funny as fuck lol     -   -    -    -  \n",
       "49999         @josiestolla now i don't do drama. bounce.     -   -    -    -  \n",
       "\n",
       "[50000 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/BioCreative_TrainTask3.0.tsv', sep='\\t')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "posdf = train_data.loc[train_data['start'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(zip(posdf['text'].to_numpy(), posdf['start'].to_numpy(), posdf['end'].to_numpy() \\        , posdf['span'].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 @\n",
      "1 C\n",
      "2 h\n",
      "3 o\n",
      "4 c\n",
      "5 o\n",
      "6 l\n",
      "7 a\n",
      "8 t\n",
      "9 e\n",
      "10 _\n",
      "11 j\n",
      "12 a\n",
      "13 d\n",
      "14 e\n",
      "15 _\n",
      "16  \n",
      "17 o\n",
      "18 h\n",
      "19  \n",
      "20 l\n",
      "21 a\n",
      "22 w\n",
      "23 d\n",
      "24 üò≠\n",
      "25 üò≠\n",
      "26  \n",
      "27 t\n",
      "28 h\n",
      "29 a\n",
      "30 t\n",
      "31 '\n",
      "32 s\n",
      "33  \n",
      "34 k\n",
      "35 i\n",
      "36 n\n",
      "37 d\n",
      "38 a\n",
      "39  \n",
      "40 w\n",
      "41 h\n",
      "42 y\n",
      "43  \n",
      "44 I\n",
      "45  \n",
      "46 w\n",
      "47 a\n",
      "48 n\n",
      "49 t\n",
      "50  \n",
      "51 t\n",
      "52 o\n",
      "53  \n",
      "54 g\n",
      "55 e\n",
      "56 t\n",
      "57  \n",
      "58 t\n",
      "59 h\n",
      "60 e\n",
      "61  \n",
      "62 e\n",
      "63 p\n",
      "64 i\n",
      "65 d\n",
      "66 u\n",
      "67 r\n",
      "68 a\n",
      "69 l\n",
      "70  \n",
      "71 üò©\n",
      "72  \n",
      "73 s\n",
      "74 o\n",
      "75  \n",
      "76 I\n",
      "77  \n",
      "78 w\n",
      "79 o\n",
      "80 n\n",
      "81 '\n",
      "82 t\n",
      "83  \n",
      "84 f\n",
      "85 e\n",
      "86 e\n",
      "87 l\n",
      "88  \n",
      "89 s\n",
      "90 t\n",
      "91 i\n",
      "92 t\n",
      "93 c\n",
      "94 h\n",
      "95 e\n",
      "96 s\n",
      "97  \n",
      "98 üòÇ\n"
     ]
    }
   ],
   "source": [
    "sample = \"@Chocolate_jade_ oh lawdüò≠üò≠ that's kinda why I want to get the epidural üò© so I won't feel stitches üòÇ\"\n",
    "for i,s in enumerate(sample):\n",
    "    print(i,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_data.text.apply(lambda x : len(x)).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-', 'zofran', 'laxatives', 'fiber gummies', 'birth control',\n",
       "       'tums', 'nyquil', 'sudafed', 'paracetamol', 'benadryl', 'advil',\n",
       "       'epidural', 'depo-provera', 'pitocin', 'IVF injection', 'tylenol',\n",
       "       'acetaminophen', 'white heat preworkout', 'lupron',\n",
       "       'prenatal vitamins', 'menopur', 'aspirin', 'concerta',\n",
       "       'painkillers', 'gaviscon', 'antibiotic', 'diclegis', 'warfarin',\n",
       "       'antibiotics', 'ambien', 'nausea pills', 'nicotine', 'vitamin B',\n",
       "       'COR creatine', 'gotu kola', 'melatonin', 'pain medicine',\n",
       "       'percocet', 'anesthetic', 'nexium', 'iron pill',\n",
       "       'hydrocortisone cream', 'zantac', 'IUD', 'vaccines', 'prenatals',\n",
       "       'vitamins', 'botox', 'baby aspirin', 'castor oil', 'flu vaccine',\n",
       "       'insulin', 'flu medication', 'steroids', 'entonox', 'pills',\n",
       "       'nitrous oxide', 'unisom'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.drug.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1562.5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
